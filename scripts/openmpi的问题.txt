openmpi-1.6.3运行出现的问题是
[shenma161:09266] mca: base: component_find: unable to open /pkg/openmpi/1.6.3/gcc-4.4/lib/openmpi/mca_mtl_mxm: libmxm.so.0: cannot open shared object file: No such file or directory (ignored)
[shenma161:09264] mca: base: component_find: unable to open /pkg/openmpi/1.6.3/gcc-4.4/lib/openmpi/mca_mtl_mxm: libmxm.so.0: cannot open shared object file: No such file or directory (ignored)
[shenma161:09268] mca: base: component_find: unable to open /pkg/openmpi/1.6.3/gcc-4.4/lib/openmpi/mca_mtl_mxm: libmxm.so.0: cannot open shared object file: No such file or directory (ignored)


重新编译openmpi-1.6.3：

mtl_mxm_send.c: In function 'ompi_mtl_mxm_send':
mtl_mxm_send.c:96: error: 'mxm_wait_t' undeclared (first use in this function)
mtl_mxm_send.c:96: error: (Each undeclared identifier is reported only once
mtl_mxm_send.c:96: error: for each function it appears in.)
mtl_mxm_send.c:96: error: expected ';' before 'wait'
mtl_mxm_send.c:104: error: 'MXM_REQ_FLAG_BLOCKING' undeclared (first use in this function)
mtl_mxm_send.c:118: error: 'MXM_REQ_FLAG_SEND_SYNC' undeclared (first use in this function)
mtl_mxm_send.c:134: error: 'wait' undeclared (first use in this function)
mtl_mxm_send.c: In function 'ompi_mtl_mxm_isend':
mtl_mxm_send.c:183: error: 'MXM_REQ_FLAG_SEND_SYNC' undeclared (first use in this function)
make[2]: *** [mtl_mxm_send.lo] Error 1
make[2]: *** Waiting for unfinished jobs....
make[2]: Leaving directory `/tmp/liuxj-build/openmpi-1.6.3/ompi/mca/mtl/mxm'
make[1]: *** [all-recursive] Error 1

编译命令： ./configure  CC=gcc   CXX=g++ F77=gfortran FC=gfortran --prefix=/install/build/liuxj/$PRG_ENV/openmpi-1.6.3  --with-openib=/usr --enable-openib-connectx-xrc --enable-mpi-thread-multiple --with-threads --with-hwloc --enable-heterogeneous --with-mxm=/opt/mellanox/mxm --with-mxm-libdir=/opt/mellanox/mxm/lib  --enable-binaries --with-devel-headers  --with-fca=/opt/mellanox/fca


编译的时候不用mxm的参数，可以编译安装，但是运行程序的时候出现warning：
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              shenma156
  Registerable memory:     4096 MiB
  Total memory:            65512 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
